# Thalex Rust SDK — Performance Addendum (6 точечных оптимизаци�� + разбор audit/)

**Контекст:** Проект используется как закрытое ПО, фокус только на производительности.  
**Ограничение:** рефакторинг SDK/публичного API невозможен — оптимизации допускаются только внутри `src/` (и при необходимости минимальные правки `Cargo.toml` для зависимостей).

---

## 0) Что я проверил в коде `src/`

Критический путь в текущей реализации находится в `src/ws_client.rs`:

- `run_single_connection()` → чтение `ws.next()` → `handle_incoming(...)`
- `handle_incoming()` делает **полный `serde_json::from_str::<Value>` для каждого сообщения**
- далее:
  - RPC path: `pending_requests.lock().await` → `remove(id)` → `tx.send(text)`
  - Sub path: `subscriptions.lock().await` → `get_mut(channel)` → `sender.send(text)` → cleanup

Также есть два явно лишних копирования:
- `handle_incoming(text.to_string(), ...)` при `Message::Text(text)` (т.к. `text` уже `String`)
- `String::from_utf8(bin.to_vec())` при `Message::Binary(bin)` (копирует буфер)

---

## 1) Детальный разбор файлов из `audit/`

Ниже — по сути “code review” выводов Cursor-а: где **согласен**, где нужна **поправка/оговорка**, и что **я бы добавил**.

### 1.1 `audit/FINAL_REPORT.md` — общий вывод

**Согласен с тезисами:**
- “Mutex в горячем пути” — верно: `pending_requests.lock()` и `subscriptions.lock()` вызываются на каждое входящее сообщение (как минимум на messages с `id` или `channel_name`).
- “JSON парсится полностью для каждого сообщения” — верно: всегда парсим в `serde_json::Value`, даже если затем используем 1–2 поля.
- “Батчинга переподписок нет” — верно: при реконнекте отправляются N subscribe-сообщений на N каналов.

**Оговорка к “конкурентная деградация линейна — критично”:**
- в реальном рантайме **входящие сообщения читаются одним циклом** (`ws.next()` + `handle_incoming()`), т.е. обработка по сути последовательна.
- деградация “по числу каналов” в бенче проявляется сильнее из‑за искусственной конкуренции задач, однако contention всё равно существует между:
  - потоком обработки входящих сообщений
  - вызовами `subscribe/unsubscribe/call_rpc`, которые тоже берут эти же mutex-ы.

Итого: вывод о bottleneck верный, но степень “линейной катастрофы” нужно интерпретировать через реальный профиль нагрузки (частота подписок/отписок/параллельных RPC).

---

### 1.2 `audit/benchmark_results_analysis.md` — корректность интерпретации чисел

**Сильная часть отчёта:** сравнения порядка величин полезны:
- RPC path ~300ns выглядит правдоподобно (простая маршрутизация + очень короткий критический участок).
- Ticker parse ~2.2µs тоже типично для `serde_json::Value` на сравнительно небольшом сообщении.

**Критическая поправка к “fast key check = contains("\"id\":\"")”:**
- в вашем коде (и в нормальном JSON-RPC) поле `id` — **число**, то есть в строке это обычно `"id":123`, а не `"id":"123"`.
- проверка `text.contains(r#""id":"#)` / `find(r#""id":"#)` из отчёта может **не сработать вообще** на реальных сообщениях.
- корректный маркер как минимум: `\"id\":` (с учётом возможных пробелов/форматирования лучше искать `"id"` и двоеточие).

**Риск ложноположительных срабатываний:**
- `contains()` может найти `"id":` внутри вложенных объектов (например, в payload уведомления), что отправит сообщение на “RPC-ветку”, хотя это не RPC response.
- это исправляется “двухэтапно”: быстрый check → затем лёгкий “envelope parsing” (см. оптимизацию #4 ниже), который проверит, что `id` действительно top-level.

---

### 1.3 `audit/optimization_recommendations.md` — что хорошо и что стоит скорректировать

**Хорошо/реалистично:**
- идея заменить `Mutex<HashMap>` на `DashMap` **для subscriptions** (read-heavy) — в целом применима без изменения публичного API.
- батчинг переподписок — даёт заметный выигрыш при реконнекте и снижает нагрузку.
- backoff/jitter — полезно (хотя это больше про reliability/UX, чем про “nanoseconds”).

**Что нужно поправить:**
1) `find(r#""id":"#)` — см. выше: почти наверняка неверный маркер для numeric id.
2) пример батчинга переподписок в отчёте держит `subs = subscriptions.lock().await;` и затем делает `ws.send(...).await` — это удержание mutex-а на время I/O.
   - в текущем `src/` у вас ровно так же (lock удерживается во время `.await`).
   - правильный вариант: **снять snapshot ключей под lock**, отпустить lock, потом отправлять.
3) предложение “изменить `handle_incoming(text: &str)`” — полезно, но аккуратно: в реальном коде вы иногда передаёте владение `text` в `tx.send(text)`/`sender.send(text)`. Нужна стратегия “clone only on the path where it’s actually needed”.

---

### 1.4 `audit/implementation_examples.md` — применимость

Файл полезен как черновик, но его нужно воспринимать как “идеи”, не как готовый patch:
- часть примеров полагается на `DashMap` + `.get_mut()` и работу с `RefMut`; это требует осторожного использования, чтобы не держать guard во время “долгой” операции (хоть `send()` и не `await`, это всё равно время под shard‑lock).
- часть примеров не учитывает “hold lock across await” в resubscribe.

---

## 2) Дополнительный отчёт: 6 точечных оптимизаций (без рефакторинга SDK)

Ниже — **ровно 6** изменений, которые можно сделать *внутри `src/`*, не меняя внешний API.

### Оптимизация 1 — убрать лишние копии входящих сообщений (самый дешёвый выигрыш)

**Где сейчас:**
- `Message::Text(text)` → `handle_incoming(text.to_string(), ...)`
- `Message::Binary(bin)` → `String::from_utf8(bin.to_vec())`

**Почему важно:** это лишние аллокации/копирования на каждом сообщении (ticker может быть очень частым).

**Что сделать:**
- передавать `text` напрямую (он уже `String`)
- `String::from_utf8(bin)` без `to_vec()`

**Ожидаемый эффект:** снижение alloc/копий (в профиле часто видно сразу).

---

### Оптимизация 2 — не держать `subscriptions` lock во время `.send()` (уменьшить время удержания mutex)

**Где сейчас:** в `handle_incoming()` sub-ветка держит `subs.lock().await` до конца обработки.

**Что сделать:**
1) Под lock взять `sender.clone()` (UnboundedSender клонируется дёшево).
2) Drop lock.
3) `sender.send(text)` вне lock.
4) Если send failed → коротко взять lock и удалить entry (опционально: “remove if still same sender”).

**Почему:** так вы уменьшаете contention между:
- входящим обработчиком
- `subscribe/unsubscribe`, которые тоже хотят lock.

---

### Оптимизация 3 — для RPC path: `remove(id)` под lock, а `tx.send(text)` вне lock

Аналогично оптимизации 2, но для `pending_requests`:

**Сейчас:** lock удерживается пока вы делаете `tx.send(...)`.

**Что сделать:**
- под lock сделать `let tx = pending.remove(&id);`
- drop lock
- `tx.send(text)` вне lock

**Почему:** снижает задержку “всем остальным” операциям, которые хотят pending lock (включая `call_rpc`).

---

### Оптимизация 4 — заменить “parse into Value always” на лёгкий envelope-parsing

**Цель:** убрать тяжёлое построение `serde_json::Value`, когда нужны только `id`/`channel_name`.

**Лучший компромисс без зависимости:** маленький struct:

```rust
#[derive(Deserialize)]
struct Envelope<'a> {
    id: Option<u64>,
    #[serde(borrow)]
    channel_name: Option<std::borrow::Cow<'a, str>>,
}
```

**Паттерн:**
- сначала быстрый check (`memchr`/`contains`) чтобы отсеять “совсем неинтересные” сообщения
- затем `serde_json::from_str::<Envelope>(&text)`
- если `id.is_some()` → RPC path
- иначе если `channel_name.is_some()` → sub path
- иначе warn/ignore

**Почему лучше, чем `Value`:**
- меньше аллокаций
- меньше CPU на построение дерева
- меньше pressure на allocator

**Важная поправка к audit:** искать надо не `""id":"` а хотя бы `"id":` (numeric), и лучше потом подтвердить через `Envelope`.

---

### Оптимизация 5 — resubscribe: snapshot keys под lock + батчинг (и не держать lock во время await)

**Сейчас:**
- держим `subscriptions.lock().await`
- в цикле делаем `ws.send(...).await?` (I/O под lock)

**Что сделать:**
1) Под lock собрать `Vec<String>` каналов (snapshot).
2) Drop lock.
3) Отправить либо:
   - один batched subscribe (если сервер принимает `channels: [...]`)
   - либо fallback: loop без lock

**Плюс:** уменьшение времени удержания lock + ускорение реконнекта.

---

### Оптимизация 6 — предварительное резервирование HashMap ёмкости (и cheap housekeeping)

Это “мелочь”, но она не ломает API и часто окупается на долгоживущих процессах:

- `pending_requests`: можно `HashMap::with_capacity(N)` если есть ожидаемая верхняя граница параллельных RPC (например 1024).
- `subscriptions`: `HashMap::with_capacity(M)` (например 128/256).
- при `subscribe()` если канал уже существует — не перетирать sender молча (иначе “двойная подписка” оставляет старую таску жить). Можно:
  - вернуть Err/лог warning и заменить аккуратно, либо
  - снять старый sender и дать ему завершиться.

Это скорее **производительность и контроль ресурсов**, чем “наносекунды”.

---

## 3) Приоритизация внедрения (минимум риска → максимум профита)

1) Опт.1 (копии) — безопасно и быстро.
2) Опт.5 (resubscribe snapshot + batching) — безопасно, уменьшает lock+await проблему.
3) Опт.2/3 (send вне lock) — также безопасно, обычно даёт заметный эффект при конкуренции.
4) Опт.4 (Envelope вместо Value) — средний риск (нужно покрыть тестами), но большой выигрыш.
5) Опт.6 (capacity + housekeeping) — низкий риск.

---

## 4) Что я бы попросил прогнать после изменений (в рамках уже существующих `benches/`)

Чтобы сравнение “до/после” было честным:
- `handle_incoming` bench: отдельно для RPC и subscription path, с реальным JSON форматом `id` как числом.
- JSON parsing bench: добавить `Envelope` parsing рядом с `Value`.
- Throughput bench: держать реалистичную модель (один reader-loop) + отдельный stress test на параллельные `subscribe/unsubscribe/call_rpc`.

---

## 5) Итог

С выводами аудита в целом **согласен** (mutex contention + full JSON parse + no batching).  
Но часть конкретных “быстрых проверок” в отчётах нужно **исправить под реальный формат JSON** (`id` как число) и обязательно убрать “lock held across await” в resubscribe — это одна из самых практических проблем в текущем `src/`.

Если нужно, я могу сделать второй документ‑дополнение с **точечными патч‑диффами** для `src/ws_client.rs` (без изменения публичного API), чтобы их можно было применить руками.
