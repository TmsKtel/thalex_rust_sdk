# Анализ производительности и узких мест

**Последнее обновление:** Январь 2025  
**См. также:** 
- [thalex_rust_sdk_performance_reaudit_2025.md](./thalex_rust_sdk_performance_reaudit_2025.md) - **актуальный полный отчет о переанализе**
- [FINAL_REPORT.md](./FINAL_REPORT.md) - итоговый отчет с рекомендациями

**Примечание:** Этот документ описывает общие паттерны производительности. Для актуальных деталей и конкретных рекомендаций см. `thalex_rust_sdk_performance_reaudit_2025.md`. Файлы `*_recheck_report_*` и `*_perf_addendum_*` являются историческими документами.

## Выявленные узкие места

### 1. Блокировки Mutex в горячих путях

**Проблема:**
- В функции `handle_incoming()` происходит блокировка Mutex для каждого входящего сообщения
- Это критический путь, так как все сообщения проходят через эту функцию
- При высокой частоте сообщений (например, ticker с задержкой 100ms) блокировки могут создавать очередь
- Код использует две отдельные таблицы подписок: `public_subscriptions` и `private_subscriptions`

**Локации:**
- `handle_incoming()`: блокировки для доступа к `pending_requests` и `public_subscriptions`/`private_subscriptions` (две отдельные таблицы подписок)
- `send_rpc()`: блокировки при добавлении/удалении запросов
- `subscribe_channel()`: блокировки при управлении подписками

**Влияние:**
- Задержка обработки каждого сообщения из-за ожидания блокировки
- Потенциальная деградация при большом количестве одновременных RPC запросов
- Конкуренция между обработкой входящих сообщений и управлением подписками

### 2. Избыточное клонирование строк

**Проблема:**
- В цикле чтения WebSocket, `handle_incoming(text.to_string(), ...)` выполняет лишний клон `String`.
- В ветке обработки binary, `String::from_utf8(bin.to_vec())` выполняет лишнее копирование буфера.
- В обработке subscribe/unsubscribe, `channel.to_string()` может аллоцировать, когда вход уже является owned `String` (проверить фактические места вызова).

**Влияние:**
- Лишние аллокации памяти
- Копирование данных вместо перемещения
- Дополнительная нагрузка на GC (хотя в Rust нет GC, но аллокации все равно дорогие)

### 3. JSON парсинг на каждое сообщение

**Проблема:**
- В `handle_incoming()` каждый входящий текст парсится в `serde_json::Value`
- Даже если сообщение не требует полного парсинга (например, нужно только проверить наличие поля "id" или "channel_name")
- Бенчмарки показывают: быстрая проверка `contains("\"id\":")` в 44 раза быстрее полного парсинга
- Примечание: `id` в JSON-RPC обычно число (или `null`), поэтому проверяем маркер `"id":` и затем используем `as_u64()`

**Влияние:**
- Парсинг JSON - CPU-интенсивная операция
- При высокой частоте сообщений это может стать узким местом
- Избыточный парсинг, если нужны только определенные поля

### 4. HashMap операции в критическом пути

**Проблема:**
- Поиск в HashMap под блокировкой для каждого сообщения
- При большом количестве подписок или pending запросов поиск может замедляться
- Использование `String` как ключа требует хеширования строки

**Влияние:**
- O(1) в среднем, но может деградировать при коллизиях
- Хеширование строк - относительно дорогая операция

### 5. Отсутствие батчинга при переподписке

**Проблема:**
- В `resubscribe_all()` при переподключении отправляется отдельное сообщение для каждого канала
- Можно было бы отправить одну команду с массивом всех каналов

**Влияние:**
- Больше сетевых round-trips
- Больше сериализации JSON
- Медленнее восстановление подписок

**Примечание:** ✅ В текущем коде уже исправлена проблема "lock across await" - делается snapshot ключей под lock (для `public_subscriptions` и `private_subscriptions` отдельно), а затем await выполняется без lock. Остается проблема батчинга - отправка по одному каналу вместо одного запроса со всеми каналами.

### 6. Фиксированная задержка переподключения

**Проблема:**
- В `connection_supervisor()` используется фиксированная задержка 3 секунды
- Нет экспоненциального backoff
- Нет jitter для предотвращения thundering herd

**Влияние:**
- При проблемах с сетью может создавать излишнюю нагрузку
- Медленнее восстановление при временных проблемах

### 7. Создание отдельной задачи для каждого callback

**Проблема:**
- В `subscribe_channel()` для каждой подписки создается отдельная tokio задача
- При большом количестве подписок это создает много задач

**Влияние:**
- Дополнительный overhead на управление задачами
- Больше контекстных переключений

### 8. Отсутствие пула для переиспользования буферов

**Проблема:**
- Каждое сообщение создает новые строки и буферы
- Нет переиспользования памяти

**Влияние:**
- Больше аллокаций
- Больше pressure на аллокатор

### 9. Блокировки Mutex для instruments_cache

**Проблема:**
- В `WsClient` появился `instruments_cache: Arc<Mutex<HashMap<String, Instrument>>>`
- При частом обращении к `round_price_to_ticks()` или `cache_instruments()` может создавать contention
- Lock удерживается во время операций с кешем

**Влияние:**
- Дополнительный contention point при частом использовании
- Может блокировать другие операции при обновлении кеша

**Локации:**
- `cache_instruments()`: lock для очистки и заполнения кеша
- `round_price_to_ticks()`: lock для чтения из кеша

## Метрики для измерения

Рекомендуется добавить метрики для:
1. Время удержания блокировок Mutex
2. Количество сообщений в секунду
3. Задержка обработки сообщений (latency)
4. Количество аллокаций
5. CPU usage при высокой нагрузке

## Приоритеты оптимизации

### Высокий приоритет:
1. Оптимизация блокировок в `handle_incoming` (использование RwLock или lock-free структур)
2. Батчинг переподписок при переподключении
3. Оптимизация JSON парсинга (lazy parsing или streaming parser)

### Средний приоритет:
4. Уменьшение клонирования строк
5. Экспоненциальный backoff для переподключения
6. Использование более эффективных структур данных (например, `DashMap` для concurrent access)

### Низкий приоритет:
7. Пул буферов
8. Оптимизация управления задачами для callbacks

